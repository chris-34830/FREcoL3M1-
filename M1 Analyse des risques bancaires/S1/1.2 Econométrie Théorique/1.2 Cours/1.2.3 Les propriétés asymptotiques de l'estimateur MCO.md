## **Notions de convergence de variables alÃ©atoires**
### **La convergence de variable alÃ©atoires**
- Utilisation de rÃ¨gles de convergence spÃ©cifique pour des variables alÃ©atoires...
- Comportement Ã  la limite d'une sÃ©quence de variables alÃ©atoires : $(Z_{1},Z_{2},\dots,Z_{N}) \implies {Z_{N}}$
- Quand $N \rightarrow \infty$, est-ce que cette sÃ©quence converge ?
	Vers une valeur fixe ? $\rightarrow$ convergence
	Vers une variable alÃ©atoire ? $\rightarrow$ convergence
	Vers une distribution connue ? $\rightarrow$ thÃ©orÃ¨me central limite
##### **DÃ©finition 1: Convergence en probabilit**Ã©
> Une sÃ©quence de variables alÃ©atoires $\{Z_N\}$ converge en probabilitÃ© vers une constante (non-alÃ©atoire) $\alpha$ si, pour tout $\epsilon > 0$ : $$\begin{align*} \lim_{N \to \infty} \Pr(|Z_N - \alpha| > \epsilon) &= 0 \end{align*}$$
La dÃ©finition stipule qu'une sÃ©quence de variables alÃ©atoires $\{Z_N\}$ converge en probabilitÃ© vers une constante (non-alÃ©atoire) $\alpha$ si, pour tout $\epsilon > 0$, la probabilitÃ© que la valeur de $Z_N$ soit supÃ©rieure Ã  $\alpha + \epsilon$ tend vers $0$ lorsque le nombre de termes de la sÃ©quence, $n$, tend vers l'infini.

- La constante $a$ est appelÃ©e la limite en probabilitÃ© de $Z_n$.
- On note : $Z_N \overset{p}\to \alpha$ ou $\text{p}\lim Z_n = \alpha$ =>  $\text{p}\lim Z_n = \alpha$ indique que $Z_n$ converge en probabilitÃ© vers $\alpha$.
	La dÃ©finition est gÃ©nÃ©ralisÃ©e Ã  un vecteur ou une matrice de variables alÃ©atoires en stipulant que chacun de leurs Ã©lÃ©ments converge en probabilitÃ© vers une constante propre.

##### **DÃ©finition 2: Convergence presque sÃ»re**
>Une sÃ©quence de variables alÃ©atoires $\{Z_n\}$ converge presque sÃ»rement vers une constante (non-alÃ©atoire) $\alpha$ si :
$\Pr(\lim_{N \to \infty} Z_N = \alpha) = 1$
La dÃ©finition stipule qu'une sÃ©quence de variables alÃ©atoires $\{Z_N\}$ converge presque sÃ»rement vers une constante (non-alÃ©atoire) $\alpha$ si, pour tout Ã©vÃ©nement $\mathrm{A}$ tel que la probabilitÃ© que $\mathrm{A}$ se produise soit nulle, la probabilitÃ© que $Z_N$ appartienne Ã  $\mathrm{A}$ pour un nombre infini de valeurs de $N$ est Ã©galement nulle.
En d'autres termes, la convergence presque sÃ»re signifie que la valeur de $Z_N$ est Ã©gale Ã  $\alpha$ pour presque tous les $N$
- La constante $\alpha$ est appelÃ©e la limite presque sÃ»re de $Z_n$.
	On note :
	$Z_N \overset{a.s}\to \alpha$
- La convergence presque sÃ»re est souvent utilisÃ©e pour montrer que les estimations sont convergentes.
- La convergence presque sÃ»re est une notion de convergence plus "forte" que la convergence en probabilitÃ© si : $Z_N \overset{a.s}\to \alpha \implies Z_N \overset{p}\to \alpha$
- Le corollaire n'est pas toujours vrai*
> Exemple :
> Soit $Z_n = \frac{1}{n}$. La suite $\{Z_n\}$ converge en probabilitÃ© vers 0. Cependant, la suite $\{Z_n\}$ ne converge pas presque sÃ»rement vers 0, car la probabilitÃ© que $Z_n = 1$ est non nulle pour tout $n$.

##### **DÃ©finition 3: Convergence en moyenne quadratique**
>Une suite de variables alÃ©atoires $\{Z_{Nâ€‹}\}$ converge en moyenne quadratique vers une constante (non-alÃ©atoire) $\alpha$ si :
$\lim_{N \to \infty} \mathbb{E}[Z_{N}]=\alpha$ et $\lim_{N \to \infty} \mathbb{E}[(Z_N - \alpha)^2] = 0$
La dÃ©finition stipule qu'une suite de variables alÃ©atoires $\{Z_{Nâ€‹}\}$ converge en moyenne quadratique vers une constante (non alÃ©atoire) $\alpha$ si, pour tout $\epsilon > 0$, la variance de $Z_N - \alpha$ tend vers 0 lorsque le nombre de termes de la suite, $N$, tend vers l'infini.

- La constante $\alpha$ est appelÃ©e la limite en moyenne quadratique de $Z_n$.
	On note :
	$Z_N \overset{m.s}\to \alpha$
- Ainsi, si : $\lim_{N \to \infty} \mathbb{E}[Z_{N}]=\alpha$ et $\lim_{N \to \infty} \mathbb{V}[(Z_N)] = 0$, alors $Z_N \overset{m.s}\to \alpha$	
- La convergence en moyenne quadratique est une notion de convergence plus "forte" que la convergence en probabilitÃ© si : $Z_N \overset{m.s}\to \alpha \implies Z_N \overset{p}\to \alpha$
- Le corollaire n'est pas toujours vrai*
>**Rajout personnel**
La notation $\lim_{N \to \infty} \mathbb{V}[(Z_N)] = 0$ est Ã©quivalente Ã  la notation $\lim_{N \to \infty} \mathbb{E}[(Z_N - \alpha)^2] = 0$. 
Pour Ãªtre prÃ©cis ; $\lim_{N \to \infty} \mathbb{E}[(Z_N - \alpha)^2] = 0 \implies \lim_{N \to \infty} \mathbb{V}[(Z_N)] = 0$ car
$\text{Var}(Z_N - \alpha) = \mathbb{E}[(Z_N - \alpha - \mu)^2]$
$\equiv \text{Var}(Z_N - \alpha) = \mathbb{E}[Z_N^2 - 2Z_N \alpha + \alpha^2]$
$\equiv \text{Var}(Z_N - \alpha) = \mathbb{E}[Z_N^2] - 2 \alpha \mathbb{E}[Z_N] + \alpha^2$
$\equiv \lim_{N \to \infty} \mathbb{E}[Z_N^2] - 2 \alpha \mathbb{E}[Z_N] + \alpha^2 = 0$
En effet, si la variance de la suite est Ã©gale Ã  0, alors la moyenne des carrÃ©s des Ã©carts Ã  la moyenne est Ã©gale Ã  0. Par consÃ©quent, les notations sont Ã©quivalentes

##### **DÃ©finition 4: Convergence en probabilitÃ© vers une variable alÃ©atoire**
> Une sÃ©quence de variables alÃ©atoires $\{Z_N\}$ converge en probabilitÃ© vers une variable alÃ©atoire $Z$ si :
> $Z_{N}-Z\overset{p}\to 0$ alors $Z_{N}\overset{p}\to Z$
> (ou $\lim_{N \to \infty} \mathbb{P}(Z_N \in \mathcal{A}) = \mathbb{P}(Z \in \mathcal{A})$)
> La dÃ©finition stipule qu'une suite de variables alÃ©atoires $\{Z_{N}\}$ converge en probabilitÃ© vers une variable alÃ©atoire $Z$ si, pour tout Ã©vÃ©nement $\mathcal{A}$ tel que la probabilitÃ© que $Z$ appartienne Ã  $\mathcal{A}$ soit non nulle, la probabilitÃ© que $\{Z_{N}\}$â€‹ appartienne Ã  $\mathcal{A}$ tend vers la probabilitÃ© que $Z$ appartienne Ã  $\mathcal{A}$ lorsque le nombre de termes de la suite, $N$, tend vers l'infini.

- Ainsi :
	$Z_N - Z \overset{a.s}\to 0$ alors $Z_N \overset{a.s}\to Z$
	$Z_N - Z \overset{m.s}\to 0$ alors $Z_N \overset{m.s}\to Z$
- La convergence en probabilitÃ© vers une variable alÃ©atoire implique une convergence en probabilitÃ© vers une constante : $\lim_{n \to \infty} \mathbb{P}(Z_n = c) = \mathbb{P}(Z = c)$
- La convergence en probabilitÃ© est une notion de convergence plus "forte" que la convergence en distribution si : $Z_N \overset{p}\to Z \implies Z_N \overset{d}\to Z$ 

##### **DÃ©finition 5: Convergence en distribution**
> Soit une suite de variables alÃ©atoires $\{Z_n\}$ avec une fonction de distribution $F_N(Z_{N})$, on dira que $\{Z_N\}$ converge en distribution vers une variable alÃ©atoire $Z$ si la limite de la fonction de distribution de $Z_n$ est Ã©gale Ã  la fonction de distribution de $Z$ : $\lim_{n \to \infty} F_n(z) = F(z)$.

- On note :
	$Z_N \overset{d}\to Z$ ou $Z_N \overset{L}\to Z$ ou $Z_N \overset{d}\to F$
 - La convergence en distribution implique une convergence en probabilitÃ© si la sÃ©quence converge vers une constante, on note alors : $Z_N \overset{p}\to \alpha \implies Z_N \overset{d}\to \alpha$

##### **DÃ©finition 6 :  
> $F$ est appelÃ© la distribution asymptotique ou la distribution limite de $Z_{N}$

Se rÃ©fÃ©rer dÃ©finition 5

##### **Lemme 2.3. : PrÃ©servation de la convergence avec une transformation continue**
>Soit $\{Z_Nâ€‹\}$ une suite de variables alÃ©atoires convergeant en distribution vers une variable alÃ©atoire $Z$. Soit $g$ une fonction continue. Alors, la suite ${g(Z_{N})}$ (ne dÃ©pendant pas de N) converge en distribution vers la variable alÃ©atoire $g(Z)$.
>Si $Z_{N}\overset{p}\to a \implies g(Z_n) \overset{p}\to g(a)$ ou si $p\lim g (Z_{N})=g(p\lim Z_{N})$
Si : $Z_{N}\overset{d}\to Z \implies g(Z_n) \overset{d}\to g(Z)$

ConsÃ©quences du Lemme :
- $X_{N}\overset{p}\to \beta \text{ et } Y_{N}\overset{p}\to \alpha \implies X_{N}+Y_{N}\overset{p} \to \beta+\alpha$
- $X_{N}\overset{P}\to \beta \text{ et } Y_{N}\overset{p}\to \alpha \implies X_{N}Y_{N}\overset{p} \to \beta\alpha$
- $X_{N}\overset{p}\to \beta \text{ et } Y_{N}\overset{p}\to \alpha \implies \frac{X_{N}}{Y_{N}}\overset{p} \to \frac{\beta}{\alpha}$ tant que $a \ne 0$
- ${Y_{N}}\overset{p} \to \gamma \implies{Y_{N}}^{-1}\overset{p} \to \gamma^{-1}$ si $\gamma$ inversible

##### **Lemme 2.4. : PrÃ©servation de la convergence avec une transformation continue**
> Si les opÃ©rations matricielles sont possibles alors :
>$X_{N}\overset{d}\to X \text{ et } Y_{N}\overset{p}\to A \implies X_{N}+Y_{N}\overset{d} \to X+A$
 $X_{N}\overset{d}\to X \text{ et } Y_{N}\overset{p}\to 0 \implies Y_{N}^{'}X_{N}\overset{P} \to 0$
 $X_{N}\overset{d}\to X \text{ et } A_{N}\overset{p}\to A \implies A_{N}X_{N}\overset{d} \to AX$
 $X_{N}\overset{d}\to X \text{ et } A_{N}\overset{p}\to A \implies X_{N}^{'}A_{N}^{-1}X_{N}\overset{d} \to X^{'}A^{-1}X$

##### **DÃ©finition 7 : Equivalence asymptotique**
> Deux sÃ©quences $\{Z_{N}\}$ et $\{X_{N}\}$ sont asymptotiquement Ã©quivalentes si :
> $Z_{N}-X_{N}\overset{p}\to 0$
- On note :
	$Z_N \asymp X_N$ ou $Z_{N}=X_{N}+o_{p}$, avec $o_p$ une variable alÃ©atoire adÃ©quate qui converge en probabilitÃ© vers zÃ©ros.

##### **Lemme 2.5. : MÃ©thode du Delta**
> Si $\{X_{N}\}$ est une sÃ©quence de vecteurs alÃ©atoires de dimension K qui converge en probabilitÃ© vers un vecteur constant $\beta$ : $X_{N}\overset{p}\to\beta$ et $\sqrt{ N }(X_{N}-\beta)\overset{d}\to Z$
> On suppose une fonction $f(X_N)$ ou $\mathbb{R}^K \to \mathbb{P}^K$ continÃ»ment diffÃ©rentiable telle que $G(\beta) = \frac{df}{d\beta}(\beta)$ est la matrice $P \times K$ des dÃ©rivÃ©es premiÃ¨res Ã©valuÃ©e en $\beta$
> alors $\sqrt{ N }[f(X_{N}-f(\beta))]\overset{d}\to G(\beta)Z$
> Ainsi, si nous avons une sÃ©quence de vecteurs alÃ©atoires qui sont asymptotiquement normaux, et si nous appliquons une fonction continuellement diffÃ©rentiable Ã  ces vecteurs, alors la sÃ©quence rÃ©sultante de vecteurs alÃ©atoires sera Ã©galement asymptotiquement normale.


### **Les estimateurs sont des sÃ©quences de variables alÃ©atoires**
On considÃ¨re $\hat{\theta_{N}}$ un estimateur d'un vecteur de paramÃ¨tres $\theta$ Ã  partir d'un Ã©chantillon de taille $N$.
En augmentant la taille de l'Ã©chantillon, on a une sÃ©quence de variables alÃ©atoires : $\{\hat{\theta_{N}}\}$.
##### **DÃ©finition 8 : Convergence d'un estimateur**
>Un estimateur d'un vecteur de paramÃ¨tres est convergent (consistent) si et seulement si $\lim_{N\to\infty} \mathbb{E_\theta}[\|\hat\theta_N - \theta\|] = 0$ qui est la notation de $\hat{\theta_{N}}\overset{p}\to\theta$
- On parle parfois de Â«convergence faibleÂ» par opposition Ã  la Â«< convergence forte Â» lorsque le concept de convergence presque sÃ»re est utilisÃ©e...
##### **DÃ©finition 9 : Biais asymptotique**
> Le biais asymptotique d'un estimateur d'un vecteur de paramÃ¨tres $Î¸$ est dÃ©fini par :
> $As.Biais(\hat{\theta_N}) = \lim_{N \to \infty} \mathbb{E}(\hat{\theta_N}) - \theta$

Donc un estimateur est convergent si son biais asymptotique est nul.
##### **DÃ©finition 10 : Estimateur convergent et asymptotiquement normal**
>Un estimateur $\hat\theta_N$â€‹ d'un vecteur de paramÃ¨tres $Î¸$ est dit convergent et asymptotiquement normal si 
>  $\hat\theta_N$â€‹ est convergent (consistent) donc $\hat\theta_N \xrightarrow{p} \theta$,
> $\hat\theta_N$ est asymptotiquement normal donc  $\sqrt{N}(\hat\theta_N-\theta) \xrightarrow{d} N(0, \Sigma).$ 
> La seconde partie signifie que la sÃ©quence de variables alÃ©atoires $\sqrt{N}(\hat\theta_N-\theta)$ converge vers une distribution normale de moyenne $0$ et de variance $Î£$, lorsque la taille de l'Ã©chantillon $N$ augmente Ã  l'infini. Se rÃ©fÃ©rer Ã  la dÃ©finition 5

### **Loi des grands nombres**
> Soit $Z_1, Z_2, ..., Z_N$ une suite de variables alÃ©atoires indÃ©pendantes et identiquement distribuÃ©es (i.i.d.) de moyenne $\mu$ et de variance $\sigma^2$. Alors, $\frac{1}{N} \sum_{i=1}^N Z_i \xrightarrow{a.s.} \mu$. On parle de **forme forte**.
>Soit $X_1, X_2, ..., X_N$ une suite de variables alÃ©atoires indÃ©pendantes et identiquement distribuÃ©es (i.i.d.) de moyenne $\mu$ et de variance $\sigma^2$. Alors, $\frac{1}{N} \sum_{i=1}^N X_i \xrightarrow{p} \mu$. On parle de **forme faible**.

##### **Loi faible des grands nombres de Khintchine**
>Soit une suite de variables alÃ©atoires ${Z_Nâ€‹}$ i.i.d. (indÃ©pendantes et identiquement distribuÃ©es) avec espÃ©rance $Î¼$ et variance finie. Alors, la moyenne empirique $\bar{Z_nâ€‹}=\frac{1}{n}â€‹âˆ‘_{i=1}^{n}Z_iâ€‹$ converge en probabilitÃ© vers $Î¼$, c'est-Ã -dire que pour tout $Ïµ>0$, $P\left(\left|\overline{Z}_n - \mu\right| \geq \epsilon\right) \to 0 \text{ lorsque } n \to \infty.$

Preuve (diffÃ©rente du cours pour que ma notation soit constante):
	En utilisant l'inÃ©galitÃ© de Tchebychev-BienaymÃ©, on obtient que pour tout $\epsilon > 0$, $P\left(\left|\overline{Z}_n - \mu\right| \geq \epsilon\right) \leq \frac{\text{Var}(Z_1)}{n \epsilon^2}.$
	En prenant la limite supÃ©rieure lorsque $n \to \infty$, on obtient que $\lim_{n \to \infty} P\left(\left|\overline{Z}_n - \mu\right| \geq \epsilon\right) \leq \frac{\text{Var}(Z_1)}{\epsilon^2}.$
	Puisque $\text{Var}(Z_1)$ est fini, on a que $\lim_{n \to \infty} P\left(\left|\overline{Z}_n - \mu\right| \geq \epsilon\right) = 0$

##### **Loi faibles des grands nombre de Tchebychev**
>Soit une suite de variables alÃ©atoires $\{Z_n\}$ i.i.d. (indÃ©pendantes et identiquement distribuÃ©es) avec espÃ©rance $\mu$ et variance $\sigma^2$. Alors, pour tout $\epsilon > 0$, $P\left(\left|\frac{1}{n} \sum_{i=1}^n Z_i - \mu\right| \geq \epsilon\right) \leq \frac{\sigma^2}{\epsilon^2 n}.$ qui n'est rien d'autre que $P(|\overline{Z}_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2 n}$ 
La loi faible des grands nombres de Tchebychev nous dit que la moyenne d'Ã©chantillon $Z_{N}$â€‹ **converge en probabilitÃ©** vers la moyenne de la population $Î¼$ lorsque la taille de l'Ã©chantillon $N$ tend vers l'infini.

### **ThÃ©orÃ¨mes central limite**
##### **ThÃ©orÃ¨me central-limite de Lindeberg-LÃ©vy**
>Si univariÃ©e : Soit une sÃ©quences de variables alÃ©atoires {Znâ€‹} i.i.d. (indÃ©pendantes et identiquement distribuÃ©es) avec espÃ©rance $Î¼$ et variance $Ïƒ^2$. Alors, $\sqrt{N} \left( \frac{1}{N} \sum_{i=1}^N \frac{Z_i - \mu}{\sigma} \right) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$
> Si multivariÃ© alors :
> Soit une suite de variables alÃ©atoires {Znâ€‹} i.i.d. (indÃ©pendantes et identiquement distribuÃ©es) avec espÃ©rance $Î¼$ et variance $V(Z_{i})=\Sigma>0$. Alors, $\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n Z_i - \mu \right) \xrightarrow{d} \mathcal{N}(0, \Sigma)$

##### **ThÃ©orÃ¨me central-limite de Lindeberg-Feller**
>Soit une suite de variables alÃ©atoires {Znâ€‹} i.i.d. (indÃ©pendantes et identiquement distribuÃ©es) avec espÃ©rance $Î¼$ et variance $Ïƒ^2$. 
>Soit les moyennes des espÃ©rances et variances : $\overline{\mu_{N}}=\frac{1}{n} \sum_{i=1}^n \mu_i$ et ^$\overline{\sigma_{N}^2}=\frac{1}{n} \sum_{i=1}^n \sigma_i^{2}$.
>Supposons la moyenne des variances convergente vers une limite finie et dÃ©finie positive, avec aucune variance d'une observation ne dominant la somme des variances : $\lim_{N \to \infty}(N\overline{\sigma_{N}^2})^{-1}\sigma^{2}=\lim_{N \to \infty}(\sum_{i=1}^n\sigma^{2}_{i})^{-1}\sigma^{2}_{i}=0$
>Alors,$\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n Z_i - \mu \right) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$

## **Concepts fondamentaux avec des sÃ©ries temporelles.**
### **Les processus stochastiques**
En Ã©tudiant les sÃ©ries temporelles, il est nÃ©cessaire d'utiliser de nouvelle propriÃ©tÃ© asymptotiques. Les variables alÃ©atoires sont dÃ©sormais dÃ©pendantes de leur passÃ©s.
	Dans cette partie, nous ne parlerons plus de suites de variables (sÃ©quences) mais d'un processus stochastique.
- Une rÃ©alisation de ce processus stochastique est l'assignation Ã  chaque pÃ©riode $t \in T$ d'une valeur possible de $Z_{T}$
- Cette realisation sera appelÃ©e une trajectoire
- Il n'est possible d'observer qu'une seule rÃ©alisation du processus stochastique
- Le processus stochastique va Ãªtre supposÃ© stable pour permettre Ã  chaque observation temporelle d'Ãªtre considÃ©rÃ© comme une rÃ©alisation.
	Sera dÃ©finie par la stationnaritÃ©, l'ergodicitÃ©, le martingale.

##### **DÃ©finition 11 : Processus stationnaire**
> Un processus stochastique ${X_Tâ€‹}$ est dit stationnaire si sa distribution jointe est invariante par translation dans le temps. Cela signifie que la distribution de la rÃ©alisation $(X_tâ€‹,X_{t+}â€‹,X_{t+2}â€‹,\dots,X_{t_p})$ est la mÃªme que la distribution de la rÃ©alisation $(X_{t+hâ€‹},X_{t+h+1}â€‹,X_{t+h+2}â€‹,â€¦,X_{t_{p}+h})$ pour tout $hâˆˆZ$.
- Si on dÃ©finit la fonction de distribution par $F$, on aura pour un processus strictement stationnaire : $F(X_tâ€‹,X_{t+}â€‹,X_{t+2},\dots,X_{t_p})=ğ¹(X_{ğ‘¡_1+â„},X_{ğ‘¡_2+â„}, ,X_{ğ‘¡_ğ‘+â„}$ pour tout â„ âˆˆ â„¤ et fini. 
	Ce qui importe pour la distribution est sa **position relative** dans la sÃ©quence, et non sa **position absolue** dans le temps. 
	La moyenne, la variance, des moments dâ€™ordre supÃ©rieur (sâ€™ils existent) restent identiques quelle que soit la valeur du dÃ©calage $â„$ .
- Toute fonction continue d'un processus stationnaire est aussi stationnaire
- Une sÃ©rie temporelle prÃ©sentant une trend n'est pas stationnaire.
	Dans ce cas, on peut enlever la tendance (fonction de $t$), on aura alors un **processus stationnaire autour de sa tendance**
	- Si une sÃ©rie temporelle n'est pas stationnaire mais que sa diffÃ©rence premiÃ¨re l'est ($\Delta X_{t}=X_{t}-X_{t-1}$) est stationnaire, on parlera de processus stochastique $\{X_t\}$ **stationnaire en diffÃ©rence**
- Certaines sÃ©ries sont non stationnaires car leur variance augmente au cours du temps

##### **DÃ©finition 12 : Processus stationnaire en covariance**
> Un processus stochastique $\{X_T\}$ est stationnaire en covariance ou faiblement stationnaire si :
> $\mathbb{E}[X_t] = \mu$ pour tout $t = 1, 2, \dots, T$
$\mathrm{Var}[X_t] = \sigma^2 < \infty$ pour tout $t = 1, 2, \dots, T$
$\operatorname{Cov}(X_t, X_{t + s}) = f(s) = P_s$ pour tout $t = 1, 2, \dots, T$ et pour tout $s$
- $\mathbb{E}[X_t] = \mu$ signifie que l'espÃ©rance du processus est constante dans le temps.
- $\mathrm{Var}[X_t] = \sigma^2 < \infty$ signifie que la variance du processus est constante dans le temps.
- $\operatorname{Cov}(X_t, X_{t + s}) = f(s) = P_s$ (suffisante) signifie que **la covariance** entre deux rÃ©alisations du processus ne **dÃ©pend que de la diffÃ©rence entre les deux temps de rÃ©alisation**.
	A noter : **si la variance n'est pas constante dans le temps, alors la covariance entre deux rÃ©alisations du processus dÃ©pendra du temps, violant la troisiÃ¨me condition.**
- Si processus stochastique strictement stationnaire et si la variance et les covariances sont finies, alors le processus est aussi stationnaire en covariance.
	Corollaire faux.

##### **DÃ©finition 13 : Processus de bruit blanc**
>Un processus stochastique $\{X_t\}$ est un bruit blanc si :
$\mathbb{E}[X_t] = 0$ pour tout $t$
$\mathrm{Var}[X_t] = \sigma^2 < \infty$ pour tout $t$
$\operatorname{Cov}(X_t, X_{t + s}) = 0$ pour tout $t$ et pour tout $s$
Une sÃ©quence de variables alÃ©atoires indÃ©pendantes et identiquement distribuÃ©es avec une moyenne nulle est un **bruit blanc**.

##### **DÃ©finition 14 : ErgodicitÃ© d'un processus stochastique**
> Un processus stochastique $\{X_T\}$ est dit **ergodique** si, pour deux fonctions bornÃ©es quelconques $f: R^K â†’ R$ et $g:R^L â†’ R$,
> $\lim_{T\to\infty} E[f(x_t,...,x_{t+k})g(x_{t+n},...,x_{t+n+L})] = E[f(x_t,...,x_{t+k})]E[g(x_{t+n},...,x_{t+n+L})]$
> Un **processus ergodique** est un processus stochastique dont les statistiques peuvent Ãªtre approximÃ©es par lâ€™Ã©tude dâ€™une **seule rÃ©alisation du processus suffisamment longue dans le temps** (les valeurs du processus Ã  des moments Ã©loignÃ©s sont statistiquement indÃ©pendantes)

##### **ThÃ©orÃ¨me ergodique**
> Soit un processus stochastique stationnaire et ergodique $\{X_T\}$ avec $\mathbb{E}[X_t] = \mu$, alors $\overline{x_{t}}=\frac{1}{T}\sum^T_{t=1}x_{t}\overset{a.s}\to \mu$
> Implique que  la moyenne temporelle du processus convergera vers la valeur attendue du processus lorsque la pÃ©riode de temps sur laquelle la moyenne est calculÃ©e tend vers l'infini. 
- GÃ©nÃ©ralisation de la loi forte des grands nombres de Kolmogorov quand variables alÃ©atoires plus dÃ©pendantes 
- Si $\{X_T\}$ est ergodique et stationnaire, toute fonction mesurable (ou continue) donne Ã©galement un processus stationnaire et ergodique.

##### **DÃ©finition 15 : Martingale**
> Soit $x_t$ un Ã©lÃ©ment de $z_t$, le processus stochastique $\{X_T\}$ est appelÃ© une martingale par rapport au processus stochastique de $\{Z_T\}$ si : $E(X_t|Z_{t-1}, Z_{t-2}, ..., Z_1) = X_{t-1} \quad \text{pour tout } t \ge 2$
> La dÃ©finition stipule que l'espÃ©rance conditionnelle est Ã©gale Ã  sa valeur actuelle, compte tenu de toutes les observations passÃ©es. Cela signifie que, connaissant les informations disponibles Ã  un moment donnÃ©, nous ne pouvons pas prÃ©dire avec certitude la valeur future du processus.

- Les variables conditionnantes $Z_{t-1}, Z_{t-2}, ..., Z_1$ sont appelÃ©es l'**ensemble d'information Ã  la date $t-1$**.
- Le processus $\{X_t\}$ est appelÃ© simplement une **martingale** si l'ensemble d'information comprend uniquement les valeurs passÃ©es de $X_t$ : $X_{t-1}, X_{t-2}, ..., X_1$.
- On gÃ©nÃ©ralise immÃ©diatement la notion de martingale pour un vecteur.
- La marche au hasard (ou marche alÃ©atoire) est une martingale.

##### DÃ©finition 16 : Marche alÃ©atoire
>Soit $\{X_t\}$ un processus stochastique vectoriel bruit blanc (i.i.d. d'espÃ©rance nulle et de matrice de variance-covariance finie).
Une marche alÃ©atoire $\{Z_t\}$ est la sÃ©quence de somme cumulÃ©e : $Z_1 = X_1, Z_2 = X_1 + X_2, Z_t = \sum_{s=1}^t X_s, \text{ pour } t = 1,2,..., T.$
On a : $E(Z_t|Z_{t-1}, Z_{t-2}, ..., Z_1) =E(Z_t|X_{t-1}, X_{t-2}, ..., X_1)=Z_{t-1}$
$\{X_t\}$ est un processus stochastique vectoriel bruit blanc, c'est-Ã -dire qu'il est constituÃ© d'une suite de variables alÃ©atoires indÃ©pendantes et identiquement distribuÃ©es d'espÃ©rance nulle et de matrice de variance-covariance finie.
La marche alÃ©atoire $\{Z_t\}$ est dÃ©finie comme la somme cumulÃ©e des pas alÃ©atoires $\{X_t\}$. En d'autres termes, $Z_t$ est la valeur cumulÃ©e du processus stochastique $\{X_t\}$ jusqu'Ã  l'instant $t$.

##### DÃ©finition 17 : Martingale en diffÃ©rence
> Un processus stochastique vectoriel $\{X_t\}$ est une martingale en diffÃ©rence si et seulement si son espÃ©rance conditionnelle par rapport Ã  ses valeurs passÃ©es est nulle : $E(X_t - X_{t-1} | X_{t-1}, X_{t-2}, ..., X_1) = 0 \quad \text{pour tout } t \ge 2$
> Si l'espÃ©rance de $X_t - X_{t-1}$ est nulle, alors, connaissant les valeurs passÃ©es de $X_t$, on ne peut pas prÃ©dire avec certitude la valeur future de $X_t$.
- Le processus $\{Z_T\}$ provenant de la somme cumulÃ©e d'une martingale en diffÃ©rence $\{X_T\}$, est une martingale.
- L'inverse est aussi vrai !
- Une martingale en diffÃ©rence Ã  une auto-corrÃ©lation nulle : $\operatorname{Cov}(X_t, X_{t -s}) = 0$ 