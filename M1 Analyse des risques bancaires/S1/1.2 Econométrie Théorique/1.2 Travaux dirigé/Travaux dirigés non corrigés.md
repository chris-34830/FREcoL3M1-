#### Chapitre 1 : L'estimation par moindres carrÃ©s ordinaires (MCO)

##### QCM
- L'Ã©lÃ©ment $\epsilon$ dans un modÃ¨le Ã©conomÃ©trique est habituellement appelÃ© **le terme d'erreur**. C'est une variable qui traduit les perturbations qui affectent la variable dÃ©pendante sans Ãªtre causÃ©es par la variable indÃ©pendante.
- L'Ã©lÃ©ment $\beta$ dans un modÃ¨le Ã©conomÃ©trique est habituellement appelÃ© **paramÃ¨tre**. Ils permettent de quantifier les influences des autres autres facteurs sur y. 
- Les paramÃ¨tres d'un modÃ¨le Ã©conomÃ©trique **dÃ©crivent la force de la relation entre la variable Ã©tudiÃ©e et les facteurs qui l'affectent**. 
- Les paramÃ¨tres d'un modÃ¨le Ã©conomÃ©trique sont fixes. 
- Le modÃ¨le de rÃ©gression linÃ©aire multiple est **linÃ©aire dans les paramÃ¨tres**
- L'erreur d'un modÃ¨le Ã©conomÃ©trique **incorpore les effets des variables explicatives inobservables sur la variable dÃ©pendante**
- Dans le modÃ¨le de rÃ©gression multiple $y=\beta_{0} + x_{1}\beta_{1}+x_{2}\beta_{2}+e$, $\beta_{0}$ est **l'ordonnÃ©e Ã  l'origine**
- Une variable dÃ©pendante est Ã©galement connue sous le nom de **variable de rÃ©ponse**
- On a absence de multicolinÃ©aritÃ© parfaite si **rang(X) = K**.
- On a une stricte exogÃ©nÃ©itÃ© des variables explicatives si $E(\epsilon_{t}|X) =0$
- Le critÃ¨re des moindres carrÃ©s **minimise la somme des carrÃ©s des erreurs**
- Dans l'estimation par moindres carrÃ©s, la matrice X'X est de rang-plein
- La matrice 

###### Vrai ou faux
- Le critÃ¨re des moindres carrÃ©s consiste Ã  minimiser la somme des carrÃ©s des rÃ©sidus
- Lâ€™estimation dâ€™une constante est nÃ©cessaire pour que la moyenne des rÃ©sidus soit nulle.
- La covariance de l'Ã©chantillon entre un rÃ©gresseur et les rÃ©sidus des moindres carrÃ©s ordinaires (MCO) dÃ©pend du signe du paramÃ¨tre estimÃ© pour ce rÃ©gresseur
- Les rÃ©sidus sont distribuÃ©s symÃ©triquement autour de zÃ©ro.
- Dans une rÃ©gression multiple, il y a ğ‘ âˆ’ ğ¾ degrÃ©s de libertÃ© dans les rÃ©sidus de moindres carrÃ©s ordinaires.
- 
- Le $R^2$ est le rapport de la variation expliquÃ©e par rapport Ã  la variation totale.
Vrai. Le $R^2$ reprÃ©sente la fraction de la variation de $y$ qui est expliquÃ©e par $x$ au sein de l'Ã©chantillon. 
- Le $\bar{ğ‘…}^2$ est toujours supÃ©rieur au $R^2$.
Faux, le  $\bar{ğ‘…}^2$ ne peut pas Ãªtre supÃ©rieur Ã  $R^2$. Le $\bar{ğ‘…}^2$ peut-Ãªtre au mieux Ã©gal Ã  $R^2$ quand le modÃ¨le ne comprend qu'une unique variable prÃ©dictive, cependant en ajoutant des variables prÃ©dictives on ajoute une pÃ©nalitÃ© qui ajuste l'estimation de la variance de l'erreur en fonction du nombre de degrÃ© de libertÃ©. (Augmentant avant le nombre de variable prÃ©dictive)
- Le  $\bar{ğ‘…}^2$ peut Ãªtre nÃ©gatif.
Vrai
- 

##### Exercices thÃ©oriques
- En supposant deux matrices carrÃ©es A et B de mÃªme dimension, dÃ©montrez la propriÃ©tÃ© de circularitÃ© de la trace : tr(AB) = tr(BA)
Soit $A = [a_{ij}]$ et $B =[b_{ij}]$ des matrices carrÃ© de taille n
Le produit matriciel donne $(AB)_{ij} =\sum^n_{k=1}a_{ik}b_{kj}$ et  $(BA)_{nn} =\sum^n_{k=1}b_{ik}a_{kj}$ ainsi, $tr(AB)=\sum^n_{i=1}(AB)_{ii}= \sum^n_{i=1}\sum^n_{k=1}a_{ik}b_{ki}= \sum^n_{i=1}\sum^n_{k=1}b_{ik}a_{ki}=\sum^n_{k=1}(AB)_{kk} = tr(BA)$ 
- Soit A une matrice de dimension $r \times c$ , dÃ©montre que la trace de $A^tA$ est la somme des carrÃ©s de tous les Ã©lÃ©ments de A  :$(A^tA)= \sum^c_{i=1}\sum^r_{k=1}a_{ik}^2$ 
La transposÃ© n'affecte pas la diagonale principale d'une matrice ainsi, $Tr(A^tA) = Tr(AA) =\sum^c_{i=1}\sum^r_{k=1}a_{ik}^2$ * DÃ©montrable de la mÃªme faÃ§on que $tr(AB) = tr(BA)$
- Expliquez pourquoi chacune des propositions suivantes est vraie
-$(A+A^t)B = AB+ A^tB$
pour simplifier la notation notons $A^t = C$ ainsi $(A+C)B = \sum^n_{k=1}(a+c)_{kj}b_{ik} =\sum^n_{k=1}a_{kj}b_{ik}+c_{kj}b_{ik}=(AB)_{ij}+(CB)_{ij}=(AB+CB)_{ij}$ 
Le produit matriciel est distributif.
-$tr(B^tB)=tr(BB^t)$ 
MÃªme dÃ©monstration que pour la question 1 cependant dans notre cas $A=B^t$ 
-?
-VÃ©rifiez chaque proposition avec : A = $\begin{pmatrix} -1 & 2\\ 7 & 3  \end{pmatrix}$, B = $\begin{pmatrix} 6 & 8 &-1 &0 \\ 2 & 3&1&4 \end{pmatrix}$ et $x^t=\begin{pmatrix} 1 &1&2&3\end{pmatrix}$
