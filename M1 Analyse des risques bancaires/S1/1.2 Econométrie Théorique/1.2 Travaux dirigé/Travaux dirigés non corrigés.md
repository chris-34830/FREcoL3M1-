#### Chapitre 1 : L'estimation par moindres carrÃ©s ordinaires (MCO)

##### QCM
- L'Ã©lÃ©ment $\epsilon$ dans un modÃ¨le Ã©conomÃ©trique est habituellement appelÃ© **le terme d'erreur**. C'est une variable qui traduit les perturbations qui affectent la variable dÃ©pendante sans Ãªtre causÃ©es par la variable indÃ©pendante.
- L'Ã©lÃ©ment $\beta$ dans un modÃ¨le Ã©conomÃ©trique est habituellement appelÃ© **paramÃ¨tre**. Ils permettent de quantifier les influences des autres autres facteurs sur y. 
- Les paramÃ¨tres d'un modÃ¨le Ã©conomÃ©trique **dÃ©crivent la force de la relation entre la variable Ã©tudiÃ©e et les facteurs qui l'affectent**. 
- Les paramÃ¨tres d'un modÃ¨le Ã©conomÃ©trique sont fixes. 
- Le modÃ¨le de rÃ©gression linÃ©aire multiple est **linÃ©aire dans les paramÃ¨tres**
- L'erreur d'un modÃ¨le Ã©conomÃ©trique **incorpore les effets des variables explicatives inobservables sur la variable dÃ©pendante**
- Dans le modÃ¨le de rÃ©gression multiple $y=\beta_{0} + x_{1}\beta_{1}+x_{2}\beta_{2}+e$, $\beta_{0}$ est **l'ordonnÃ©e Ã  l'origine**
- Une variable dÃ©pendante est Ã©galement connue sous le nom de **variable de rÃ©ponse**
- On a absence de multicolinÃ©aritÃ© parfaite si **rang(X) = K**.
- On a une stricte exogÃ©nÃ©itÃ© des variables explicatives si $E(\epsilon_{t}|X) =0$
- Le critÃ¨re des moindres carrÃ©s **minimise la somme des carrÃ©s des erreurs**
- Dans l'estimation par moindres carrÃ©s, la matrice X'X est de rang-plein
- La matrice $M = I - X(X^tX)^{-1}X^t$  est **symÃ©trique et idempotente**. La matrice M est une matrice de projection, par dÃ©finition elle est rÃ©elle, symÃ©trique et idempotente.
- Le levier dâ€™une observation mesure **lâ€™influence dâ€™une observation dans une rÃ©gression**
- La somme des leviers dans une rÃ©gression est Ã©gale **au nombre de variables explicatives**


###### Vrai ou faux
- Le critÃ¨re des moindres carrÃ©s consiste Ã  minimiser la somme des carrÃ©s des rÃ©sidus
**Vrai**
- Lâ€™estimation dâ€™une constante est nÃ©cessaire pour que la moyenne des rÃ©sidus soit nulle.
**Vrai**? Il semble que $E[\epsilon]=0$ dÃ©finit la constante?
- La covariance de l'Ã©chantillon entre un rÃ©gresseur et les rÃ©sidus des moindres carrÃ©s ordinaires (MCO) dÃ©pend du signe du paramÃ¨tre estimÃ© pour ce rÃ©gresseur
?
- Les rÃ©sidus sont distribuÃ©s symÃ©triquement autour de zÃ©ro.
?
- Dans une rÃ©gression multiple, il y a ğ‘ âˆ’ ğ¾ degrÃ©s de libertÃ© dans les rÃ©sidus de moindres carrÃ©s ordinaires.
**Faux**. Dans une rÃ©gression multiple, il y a N - K - 1 avec K le nombre de variables et 1 l'ordonnÃ©e Ã  l'origine.
- Le $R^2$ est le rapport de la variation expliquÃ©e par rapport Ã  la variation totale.
**Vrai**. Le $R^2$ reprÃ©sente la fraction de la variation de $y$ qui est expliquÃ©e par $x$ au sein de l'Ã©chantillon. 
- Le $\bar{ğ‘…}^2$ est toujours supÃ©rieur au $R^2$.
**Faux**. Le  $\bar{ğ‘…}^2$ ne peut pas Ãªtre supÃ©rieur Ã  $R^2$. Le $\bar{ğ‘…}^2$ peut-Ãªtre au mieux Ã©gal Ã  $R^2$ quand le modÃ¨le ne comprend qu'une unique variable prÃ©dictive, cependant en ajoutant des variables prÃ©dictives on ajoute une pÃ©nalitÃ© qui ajuste l'estimation de la variance de l'erreur en fonction du nombre de degrÃ© de libertÃ©. (Augmentant avant le nombre de variable prÃ©dictive)
- Le  $\bar{ğ‘…}^2$ peut Ãªtre nÃ©gatif.
**Vrai**
- Le $R^2$ est le coefficient de corrÃ©lation entre la variable dÃ©pendante et la variable dÃ©pendante calculÃ©e sur le plan de rÃ©gression.
**Faux**
- La matrice de projection ğ‘´ est orthogonale Ã  la matrice de projection ğ‘·.
Par construction

##### Exercices thÃ©oriques
- En supposant deux matrices carrÃ©es A et B de mÃªme dimension, dÃ©montrez la propriÃ©tÃ© de circularitÃ© de la trace : tr(AB) = tr(BA)
Soit $A = [a_{ij}]$ et $B =[b_{ij}]$ des matrices carrÃ© de taille n
Le produit matriciel donne $(AB)_{ij} =\sum^n_{k=1}a_{ik}b_{kj}$ et  $(BA)_{ij} =\sum^n_{k=1}b_{ik}a_{kj}$ ainsi, $tr(AB)=\sum^n_{i=1}(AB)_{ii}$
$=\sum^n_{i=1}\sum^n_{k=1}a_{ik}b_{ki}$
$= \sum^n_{i=1}\sum^n_{k=1}b_{ik}a_{ki}$
$=\sum^n_{k=1}(AB)_{kk} = tr(BA)$ cqfd
- Soit A une matrice de dimension $r \times c$ , dÃ©montre que la trace de $A^tA$ est la somme des carrÃ©s de tous les Ã©lÃ©ments de A  :$(A^tA)= \sum^c_{i=1}\sum^r_{k=1}a_{ik}^2$$ 
La transposÃ© ne modifie la matrice qu'en effectuant une symÃ©trie axiale des coefficients tout en conservant sa diagonale principale, ainsi la dÃ©monstration de  $Tr(A^tA) = Tr(AA) =\sum^c_{i=1}\sum^r_{k=1}a_{ik}^2$ se fait facilement : 
$(A^tA)_{ij} = \sum^r_{k=1}(A^t)_{ik}\times(A)_{kj}$
$= \sum^r_{k=1}(A)_{ik}\times(A)$
$=\sum^r_{k=1}a_{ik}\times a_{jk}$
par consÃ©quent, $(A^tA)_{ii}=\sum^r_{k=1}a^2_{ik} \implies tr(A^tA) \sum^c_{i=1}\sum^r_{k=1}a_{ik}^2=\sum_{1\le i,j \le n}a^2_{ij}$ cqfd
- Expliquez pourquoi chacune des propositions suivantes est vraie
-$(A+A^t)B = AB+ A^tB$
pour simplifier la notation notons $A^t = C$ ainsi 
$(A+C)B = \sum^n_{k=1}(a+c)_{kj}b_{ik}$
$=\sum^n_{k=1}a_{kj}b_{ik}+c_{kj}b_{ik}$ 
$=(AB)_{ij}+(CB)_{ij}=(AB+CB)_{ij}$ 
Le produit matriciel est distributif.
- $tr(B^tB)=tr(BB^t)$ 
MÃªme dÃ©monstration que pour la question 1 cependant dans notre cas $A=B^t$ 

- Traquenard sur Obsidian, faites Ã  l'Ã©crit, dÃ©monstration trop longue pour l'Ã©crire ici
-VÃ©rifiez chaque proposition avec : A = $\begin{pmatrix} -1 & 2\\ 7 & 3  \end{pmatrix}$, B = $\begin{pmatrix} 6 & 8 &-1 &0 \\ 2 & 3&1&4 \end{pmatrix}$ et $x^t=\begin{pmatrix} 1 &1&2&3\end{pmatrix}$
